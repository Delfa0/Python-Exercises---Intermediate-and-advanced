{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312154bc-5ff8-4d07-9744-98bd17921791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb71f4b-cfeb-47a3-b2a3-63669c2a72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1  # number of layers\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4794545f-ab8f-4b79-aa6b-29d7cae88aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = {}\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z = np.dot(parameters[f\"W{l}\"], A) + parameters[f\"b{l}\"]\n",
    "        A = relu(Z)\n",
    "        caches[f\"Z{l}\"] = Z\n",
    "        caches[f\"A{l}\"] = A\n",
    "\n",
    "    # Output layer (sigmoid)\n",
    "    ZL = np.dot(parameters[f\"W{L}\"], A) + parameters[f\"b{L}\"]\n",
    "    AL = sigmoid(ZL)\n",
    "    caches[f\"Z{L}\"] = ZL\n",
    "    caches[f\"A{L}\"] = AL\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476b4aa0-b491-4bff-beb7-b5cbdfc870ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    return np.squeeze(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1eeb53-3c0e-4d7b-a5d9-01fbeabf04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "def backward_propagation(parameters, caches, X, Y):\n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    AL = caches[f\"A{L}\"]\n",
    "    dZL = AL - Y\n",
    "    grads[f\"dW{L}\"] = (1 / m) * np.dot(dZL, caches[f\"A{L-1}\"].T)\n",
    "    grads[f\"db{L}\"] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(parameters[f\"W{L}\"].T, dZL)\n",
    "\n",
    "    for l in reversed(range(1, L)):\n",
    "        dZ = dA_prev * relu_derivative(caches[f\"Z{l}\"])\n",
    "        A_prev = X if l == 1 else caches[f\"A{l-1}\"]\n",
    "        grads[f\"dW{l}\"] = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "        grads[f\"db{l}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        if l > 1:\n",
    "            dA_prev = np.dot(parameters[f\"W{l}\"].T, dZ)\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabd05f6-fdb4-4142-bfeb-897bcff6d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bcd95c-2ccc-4f9e-b01f-be1540e8f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layer_dims, learning_rate=0.01, iterations=1000):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(iterations):\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propagation(parameters, caches, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}, cost: {cost:.6f}\")\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda2807d-b29e-45ea-aab2-840e7198bb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, cost: 0.693147\n",
      "Iteration 100, cost: 0.523339\n",
      "Iteration 200, cost: 0.504098\n",
      "Iteration 300, cost: 0.501077\n",
      "Iteration 400, cost: 0.500532\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "np.random.seed(2)\n",
    "X = np.random.randn(4, 5)  # 4 features, 5 examples\n",
    "Y = (np.random.randn(1, 5) > 0).astype(int)  # binary labels (0 or 1)\n",
    "\n",
    "# 4 input neurons → 3 hidden layers (3 units each) → 1 output neuron\n",
    "layer_dims = [4, 3, 3, 3, 1]\n",
    "\n",
    "# Train the model\n",
    "trained_parameters = model(X, Y, layer_dims, learning_rate=0.05, iterations=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5c2b7-8189-417a-ba0d-d1fe455ff2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
